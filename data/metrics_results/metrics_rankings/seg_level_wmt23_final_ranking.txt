lang:                                     en-de     en-de     he-en     he-en     zh-en     zh-en
level:                                      seg       seg       seg       seg       seg       seg
corr_fcn:                               pearson     acc-t   pearson     acc-t   pearson     acc-t
metric                       avg-corr     task1     task2     task3     task4     task5     task6
---------------------------  --------  --------  --------  --------  --------  --------  --------
XCOMET-Ensemble               1 0.697   1 0.695   1 0.604   1 0.556   1 0.586   1 0.650   1 0.543
MetricX-23                    2 0.682   4 0.585   2 0.603   2 0.548   2 0.577   4 0.625   4 0.531
XCOMET-QE-Ensemble[noref]     3 0.681   2 0.679   4 0.588   4 0.498   5 0.554   3 0.647   3 0.533
MetricX-23-QE[noref]          4 0.681   3 0.626   3 0.596   3 0.520   3 0.564   2 0.647   5 0.527
mbr-metricx-qe[noref]         5 0.652   5 0.571   5 0.584   6 0.411   6 0.553  10 0.489   2 0.537
GEMBA-MQM[noref]              6 0.639  10 0.502   9 0.572   7 0.401   4 0.564  12 0.449   7 0.522
MaTESe                        7 0.636   7 0.554  21 0.528   5 0.459   7 0.550   8 0.511  25 0.479
_CometKiwi[noref]             8 0.632  12 0.475  10 0.569  12 0.387   8 0.544  14 0.442   6 0.525
sescoreX                      9 0.628   8 0.519  11 0.563  13 0.385  22 0.484   7 0.536  14 0.499
SENTINEL-CAND-MQM[noref]     10 0.626   6 0.561  12 0.562  19 0.339  23 0.483   5 0.580  28 0.473
cometoid22-wmt22[noref]      11 0.625  17 0.441   6 0.578  17 0.365  15 0.515  11 0.479  10 0.515
KG-BERTScore[noref]          12 0.624  15 0.451  14 0.556  14 0.382   9 0.537  15 0.430   9 0.516
_COMET                       13 0.622  19 0.432   7 0.574   8 0.401  10 0.532  16 0.396  11 0.514
_BLEURT-20                   14 0.622  11 0.484   8 0.572  15 0.382  14 0.519  19 0.378   8 0.518
Calibri-COMET22-QE[noref]    15 0.603  18 0.441  32 0.483  11 0.395  18 0.506  13 0.443  19 0.491
Calibri-COMET22              16 0.603  21 0.413  25 0.522   9 0.401  16 0.515  17 0.396  27 0.474
_YiSi-1                      17 0.600  23 0.366  18 0.542  10 0.395  11 0.529  25 0.290  12 0.504
_docWMT22CometDA             18 0.598  22 0.394  13 0.559  18 0.339  20 0.497  23 0.353  18 0.493
_docWMT22CometKiwiDA[noref]  19 0.598  16 0.444  16 0.547  25 0.286  21 0.489  18 0.387  17 0.493
_prismRef                    20 0.593   9 0.516  29 0.518  22 0.319  12 0.528  28 0.183  13 0.504
_MS-COMET-QE-22[noref]       21 0.588  26 0.310  17 0.546  24 0.295  19 0.498  22 0.367  16 0.498
SENTINEL-CAND-DA[noref]      22 0.584  24 0.365  15 0.551  26 0.264  25 0.474  21 0.372  23 0.480
_BERTscore                   23 0.582  25 0.325  22 0.528  20 0.335  17 0.515  26 0.236  15 0.499
mre-score-labse-regular      24 0.558  36 0.111  19 0.530  16 0.378  13 0.522  30 0.145  22 0.481
XLsim                        25 0.544  29 0.239  23 0.527  30 0.233  24 0.480  32 0.111  31 0.464
_f200spBLEU                  26 0.540  30 0.237  24 0.526  31 0.230  28 0.447  33 0.108  26 0.476
MEE4                         27 0.539  34 0.202  20 0.529  27 0.256  32 0.441  34 0.105  24 0.480
tokengram_F                  28 0.537  33 0.227  26 0.520  32 0.226  26 0.461  36 0.060  21 0.485
_chrF                        29 0.537  32 0.232  28 0.519  33 0.221  27 0.460  35 0.063  20 0.485
_BLEU                        30 0.533  35 0.192  27 0.520  34 0.220  30 0.442  31 0.119  30 0.472
_prismSrc[noref]             31 0.530  20 0.425  33 0.426  36 0.140  31 0.441  27 0.223  33 0.421
embed_llama                  32 0.529  28 0.250  31 0.483  35 0.215  33 0.430  29 0.161  32 0.447
SENTINEL-SRC-MQM[noref]      33 0.512  13 0.469  38 0.231  21 0.334  38 0.428   6 0.540  38 0.240
SENTINEL-REF-MQM             34 0.506  14 0.464  36 0.231  23 0.301  36 0.428   9 0.506  36 0.240
eBLEU                        35 0.491  38-0.011  30 0.512  37 0.131  29 0.445  38-0.084  29 0.473
SENTINEL-REF-DA              36 0.477  27 0.309  35 0.231  29 0.237  35 0.428  20 0.373  35 0.240
SENTINEL-SRC-DA[noref]       37 0.469  31 0.235  37 0.231  28 0.248  37 0.428  24 0.341  37 0.240
_Random-sysname[noref]       38 0.463  37 0.064  34 0.409  38 0.041  34 0.428  37 0.018  34 0.381





\begin{tabular}{l|rr|rr|rr|rr|rr|rr|rr}
\toprule
lang: & \multicolumn{2}{|l}{} & \multicolumn{2}{|l}{en-de} & \multicolumn{2}{|l}{en-de} & \multicolumn{2}{|l}{he-en} & \multicolumn{2}{|l}{he-en} & \multicolumn{2}{|l}{zh-en} & \multicolumn{2}{|l}{zh-en} \\
level: & \multicolumn{2}{|l}{} & \multicolumn{2}{|l}{seg} & \multicolumn{2}{|l}{seg} & \multicolumn{2}{|l}{seg} & \multicolumn{2}{|l}{seg} & \multicolumn{2}{|l}{seg} & \multicolumn{2}{|l}{seg} \\
corr\_fcn: & \multicolumn{2}{|l}{} & \multicolumn{2}{|l}{pearson} & \multicolumn{2}{|l}{acc-t} & \multicolumn{2}{|l}{pearson} & \multicolumn{2}{|l}{acc-t} & \multicolumn{2}{|l}{pearson} & \multicolumn{2}{|l}{acc-t} \\
metric & \multicolumn{2}{|l}{avg-corr} & \multicolumn{2}{|l}{task1} & \multicolumn{2}{|l}{task2} & \multicolumn{2}{|l}{task3} & \multicolumn{2}{|l}{task4} & \multicolumn{2}{|l}{task5} & \multicolumn{2}{|l}{task6} \\
\midrule
XCOMET-Ensemble & \textbf{1} & \textbf{0.697} & \textbf{1} & \textbf{0.695} & \textbf{1} & \textbf{0.604} & \textbf{1} & \textbf{0.556} & \textbf{1} & \textbf{0.586} & \textbf{1} & \textbf{0.650} & \textbf{1} & \textbf{0.543} \\
MetricX-23 & 2 & 0.682 & 4 & 0.585 & 2 & 0.603 & 2 & 0.548 & 2 & 0.577 & 4 & 0.625 & 4 & 0.531 \\
XCOMET-QE-Ensemble* & 3 & 0.681 & 2 & 0.679 & 4 & 0.588 & 4 & 0.498 & 5 & 0.554 & 3 & 0.647 & 3 & 0.533 \\
MetricX-23-QE* & 4 & 0.681 & 3 & 0.626 & 3 & 0.596 & 3 & 0.520 & 3 & 0.564 & 2 & 0.647 & 5 & 0.527 \\
mbr-metricx-qe* & 5 & 0.652 & 5 & 0.571 & 5 & 0.584 & 6 & 0.411 & 6 & 0.553 & 10 & 0.489 & 2 & 0.537 \\
GEMBA-MQM* & 6 & 0.639 & 10 & 0.502 & 9 & 0.572 & 7 & 0.401 & 4 & 0.564 & 12 & 0.449 & 7 & 0.522 \\
MaTESe & 7 & 0.636 & 7 & 0.554 & 21 & 0.528 & 5 & 0.459 & 7 & 0.550 & 8 & 0.511 & 25 & 0.479 \\
\underline{CometKiwi}* & 8 & 0.632 & 12 & 0.475 & 10 & 0.569 & 12 & 0.387 & 8 & 0.544 & 14 & 0.442 & 6 & 0.525 \\
sescoreX & 9 & 0.628 & 8 & 0.519 & 11 & 0.563 & 13 & 0.385 & 22 & 0.484 & 7 & 0.536 & 14 & 0.499 \\
SENTINEL-CAND-MQM* & 10 & 0.626 & 6 & 0.561 & 12 & 0.562 & 19 & 0.339 & 23 & 0.483 & 5 & 0.580 & 28 & 0.473 \\
cometoid22-wmt22* & 11 & 0.625 & 17 & 0.441 & 6 & 0.578 & 17 & 0.365 & 15 & 0.515 & 11 & 0.479 & 10 & 0.515 \\
KG-BERTScore* & 12 & 0.624 & 15 & 0.451 & 14 & 0.556 & 14 & 0.382 & 9 & 0.537 & 15 & 0.430 & 9 & 0.516 \\
\underline{COMET} & 13 & 0.622 & 19 & 0.432 & 7 & 0.574 & 8 & 0.401 & 10 & 0.532 & 16 & 0.396 & 11 & 0.514 \\
\underline{BLEURT-20} & 14 & 0.622 & 11 & 0.484 & 8 & 0.572 & 15 & 0.382 & 14 & 0.519 & 19 & 0.378 & 8 & 0.518 \\
Calibri-COMET22-QE* & 15 & 0.603 & 18 & 0.441 & 32 & 0.483 & 11 & 0.395 & 18 & 0.506 & 13 & 0.443 & 19 & 0.491 \\
Calibri-COMET22 & 16 & 0.603 & 21 & 0.413 & 25 & 0.522 & 9 & 0.401 & 16 & 0.515 & 17 & 0.396 & 27 & 0.474 \\
\underline{YiSi-1} & 17 & 0.600 & 23 & 0.366 & 18 & 0.542 & 10 & 0.395 & 11 & 0.529 & 25 & 0.290 & 12 & 0.504 \\
\underline{docWMT22CometDA} & 18 & 0.598 & 22 & 0.394 & 13 & 0.559 & 18 & 0.339 & 20 & 0.497 & 23 & 0.353 & 18 & 0.493 \\
\underline{docWMT22CometKiwiDA}* & 19 & 0.598 & 16 & 0.444 & 16 & 0.547 & 25 & 0.286 & 21 & 0.489 & 18 & 0.387 & 17 & 0.493 \\
\underline{prismRef} & 20 & 0.593 & 9 & 0.516 & 29 & 0.518 & 22 & 0.319 & 12 & 0.528 & 28 & 0.183 & 13 & 0.504 \\
\underline{MS-COMET-QE-22}* & 21 & 0.588 & 26 & 0.310 & 17 & 0.546 & 24 & 0.295 & 19 & 0.498 & 22 & 0.367 & 16 & 0.498 \\
SENTINEL-CAND-DA* & 22 & 0.584 & 24 & 0.365 & 15 & 0.551 & 26 & 0.264 & 25 & 0.474 & 21 & 0.372 & 23 & 0.480 \\
\underline{BERTscore} & 23 & 0.582 & 25 & 0.325 & 22 & 0.528 & 20 & 0.335 & 17 & 0.515 & 26 & 0.236 & 15 & 0.499 \\
mre-score-labse-regular & 24 & 0.558 & 36 & 0.111 & 19 & 0.530 & 16 & 0.378 & 13 & 0.522 & 30 & 0.145 & 22 & 0.481 \\
XLsim & 25 & 0.544 & 29 & 0.239 & 23 & 0.527 & 30 & 0.233 & 24 & 0.480 & 32 & 0.111 & 31 & 0.464 \\
\underline{f200spBLEU} & 26 & 0.540 & 30 & 0.237 & 24 & 0.526 & 31 & 0.230 & 28 & 0.447 & 33 & 0.108 & 26 & 0.476 \\
MEE4 & 27 & 0.539 & 34 & 0.202 & 20 & 0.529 & 27 & 0.256 & 32 & 0.441 & 34 & 0.105 & 24 & 0.480 \\
tokengram\_F & 28 & 0.537 & 33 & 0.227 & 26 & 0.520 & 32 & 0.226 & 26 & 0.461 & 36 & 0.060 & 21 & 0.485 \\
\underline{chrF} & 29 & 0.537 & 32 & 0.232 & 28 & 0.519 & 33 & 0.221 & 27 & 0.460 & 35 & 0.063 & 20 & 0.485 \\
\underline{BLEU} & 30 & 0.533 & 35 & 0.192 & 27 & 0.520 & 34 & 0.220 & 30 & 0.442 & 31 & 0.119 & 30 & 0.472 \\
\underline{prismSrc}* & 31 & 0.530 & 20 & 0.425 & 33 & 0.426 & 36 & 0.140 & 31 & 0.441 & 27 & 0.223 & 33 & 0.421 \\
embed\_llama & 32 & 0.529 & 28 & 0.250 & 31 & 0.483 & 35 & 0.215 & 33 & 0.430 & 29 & 0.161 & 32 & 0.447 \\
SENTINEL-SRC-MQM* & 33 & 0.512 & 13 & 0.469 & 38 & 0.231 & 21 & 0.334 & 38 & 0.428 & 6 & 0.540 & 38 & 0.240 \\
SENTINEL-REF-MQM & 34 & 0.506 & 14 & 0.464 & 36 & 0.231 & 23 & 0.301 & 36 & 0.428 & 9 & 0.506 & 36 & 0.240 \\
eBLEU & 35 & 0.491 & 38 & -0.011 & 30 & 0.512 & 37 & 0.131 & 29 & 0.445 & 38 & -0.084 & 29 & 0.473 \\
SENTINEL-REF-DA & 36 & 0.477 & 27 & 0.309 & 35 & 0.231 & 29 & 0.237 & 35 & 0.428 & 20 & 0.373 & 35 & 0.240 \\
SENTINEL-SRC-DA* & 37 & 0.469 & 31 & 0.235 & 37 & 0.231 & 28 & 0.248 & 37 & 0.428 & 24 & 0.341 & 37 & 0.240 \\
\underline{Random-sysname}* & 38 & 0.463 & 37 & 0.064 & 34 & 0.409 & 38 & 0.041 & 34 & 0.428 & 37 & 0.018 & 34 & 0.381 \\
\bottomrule
\end{tabular}

